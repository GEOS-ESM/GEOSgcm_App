openmpi: |
  # Turn off warning about TMPDIR on NFS
  setenv OMPI_MCA_shmem_mmap_enable_nfs_warning 0
  setenv OMPI_MCA_mpi_preconnect_all 1
  setenv OMPI_MCA_coll_tuned_bcast_algorithm 7
  setenv OMPI_MCA_coll_tuned_scatter_algorithm 2
  setenv OMPI_MCA_coll_tuned_reduce_scatter_algorithm 3
  setenv OMPI_MCA_coll_tuned_allreduce_algorithm 3
  setenv OMPI_MCA_coll_tuned_allgather_algorithm 4
  setenv OMPI_MCA_coll_tuned_allgatherv_algorithm 3
  setenv OMPI_MCA_coll_tuned_gather_algorithm 1
  setenv OMPI_MCA_coll_tuned_barrier_algorithm 0
  # required for a tuned flag to be effective
  setenv OMPI_MCA_coll_tuned_use_dynamic_rules 1
  # disable file locks
  setenv OMPI_MCA_sharedfp "^lockedfile,individual"

mvapich: |
  # MVAPICH and GEOS has issues with restart writing. Having the
  # oserver write them seems to...work
  setenv MV2_ENABLE_AFFINITY 0
  setenv SLURM_DISTRIBUTION block
  setenv MV2_MPIRUN_TIMEOUT 100
  setenv MV2_GATHERV_SSEND_THRESHOLD 256

mpt: |
  setenv MPI_COLL_REPRODUCIBLE
  setenv SLURM_DISTRIBUTION block
  #setenv MPI_DISPLAY_SETTINGS 1
  #setenv MPI_VERBOSE 1
  setenv MPI_MEMMAP_OFF
  unsetenv MPI_NUM_MEMORY_REGIONS
  setenv MPI_XPMEM_ENABLED yes
  unsetenv SUPPRESS_XPMEM_TRIM_THRESH
  setenv MPI_LAUNCH_TIMEOUT 40
  setenv MPI_COMM_MAX  1024
  setenv MPI_GROUP_MAX 1024
  setenv MPI_BUFS_PER_PROC 256
  # For some reason, PMI_RANK is randomly set and interferes
  # with binarytile.x and other executables.
  unsetenv PMI_RANK
  # Often when debugging on MPT, the traceback from Intel Fortran
  # is "absorbed" and only MPT's errors are displayed. To allow the
  # compiler's traceback to be displayed, uncomment this environment
  # variable
  #setenv FOR_IGNORE_EXCEPTIONS false

intelmpi: |
  setenv I_MPI_ADJUST_ALLREDUCE 12
  setenv I_MPI_ADJUST_GATHERV 3
  # This flag prints out the Intel MPI state. Uncomment if needed
  #setenv I_MPI_DEBUG 9

NCCS: |
  # These are options determined to be useful at NCCS
  # Not setting generally as they are more fabric/cluster
  # specific compared to the above adjustments
  setenv I_MPI_SHM_HEAP_VSIZE 512
  setenv PSM2_MEMORY large

BUILT_ON_SLE15: |
  # Testing by Bill Putman found these to be
  # useful flags with Intel MPI on SLES15 on the
  # Milan nodes.
  # Note 1: Testing by NCCS shows the PSM3 provider
  # runs on the Infiniband fabric. Tests show it runs
  # up to C720.
  # Note 2: When the Cascade Lakes are moved to
  # SLES15, these will need to be Milan-only flags
  # as Intel MPI will probably work just fine with
  # Intel chips.
  setenv I_MPI_FALLBACK 0
  setenv I_MPI_FABRICS ofi
  setenv I_MPI_OFI_PROVIDER psm3
  setenv I_MPI_ADJUST_SCATTER 2
  setenv I_MPI_ADJUST_SCATTERV 2
  setenv I_MPI_ADJUST_GATHER 2
  setenv I_MPI_ADJUST_GATHERV 3
  setenv I_MPI_ADJUST_ALLGATHER 3
  setenv I_MPI_ADJUST_ALLGATHERV 3
  setenv I_MPI_ADJUST_ALLREDUCE 12
  setenv I_MPI_ADJUST_REDUCE 10
  setenv I_MPI_ADJUST_BCAST 11
  setenv I_MPI_ADJUST_REDUCE_SCATTER 4
  setenv I_MPI_ADJUST_BARRIER 9
